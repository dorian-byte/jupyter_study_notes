{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c781793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02718d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"frame\",gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efc312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df85a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65919112 0.         0.         0.         0.42075315 0.\n",
      "  0.         0.51971385 0.         0.34399327 0.        ]\n",
      " [0.         0.52210862 0.         0.         0.52210862 0.\n",
      "  0.         0.         0.52210862 0.42685801 0.        ]\n",
      " [0.         0.3218464  0.         0.50423458 0.3218464  0.\n",
      "  0.         0.39754433 0.3218464  0.52626104 0.        ]\n",
      " [0.         0.23910199 0.37459947 0.         0.         0.37459947\n",
      "  0.37459947 0.         0.47820398 0.39096309 0.37459947]]\n",
      "['blue' 'bright' 'can' 'in' 'is' 'see' 'shining' 'sky' 'sun' 'the' 'we']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assume we have the following set of documents\n",
    "documents = [\n",
    "    'The sky is blue',\n",
    "    'The sun is bright',\n",
    "    'The sun in the sky is bright',\n",
    "    'We can see the shining sun, the bright sun'\n",
    "]\n",
    "\n",
    "# Create the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the documents with the TfidfVectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# To view the TF-IDF scores\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# To view the features (unique words in all documents)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fd6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686fdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# List of image filenames\n",
    "image_filenames = [\"obama0.jpeg\", \"obama1.jpeg\", \"obama2.jpeg\", \"obama3.jpeg\", \"obama4.jpeg\", \"obama5.jpeg\"]\n",
    "\n",
    "# Create a SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "for filename in image_filenames:\n",
    "    # Load the image\n",
    "    image = cv2.imread(filename)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect and compute SIFT features\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    # Draw the keypoints on the image\n",
    "    image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)\n",
    "\n",
    "    # Display the image with keypoints\n",
    "    cv2.imshow('Image with Keypoints', image_with_keypoints)\n",
    "    print(\"Press any key to close the image window.\")\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "# Close all image windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be3fcb",
   "metadata": {},
   "source": [
    "### Feature Extraction:\n",
    "You can extract the descriptors for each image and use them as input for other machine learning algorithms. This can be helpful for tasks like image classification or clustering. Here's an example of how you can extract descriptors for each image:%macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144bc718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[51.,  4.,  1., ...,  0.,  0.,  2.],\n",
       "        [52.,  6.,  0., ...,  0.,  0.,  0.],\n",
       "        [11.,  8.,  0., ...,  0.,  0.,  7.],\n",
       "        ...,\n",
       "        [77.,  6.,  0., ...,  1.,  0.,  0.],\n",
       "        [ 9.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [78.,  5.,  0., ...,  2.,  1.,  1.]], dtype=float32),\n",
       " array([[ 3., 57., 10., ...,  0.,  0.,  2.],\n",
       "        [ 0.,  0.,  9., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ..., 44., 56., 14.],\n",
       "        ...,\n",
       "        [13.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 7.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 9.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 85.,  41.,   3., ..., 102.,  12.,   1.],\n",
       "        [ 74.,   2.,   0., ...,   6.,   0.,   0.],\n",
       "        [ 51.,   5.,   1., ...,  10.,   0.,   0.],\n",
       "        ...,\n",
       "        [  1.,   0.,   1., ...,   0.,   0.,   0.],\n",
       "        [  2.,   0.,   0., ...,   1.,  12.,  30.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.]], dtype=float32),\n",
       " array([[ 0., 25., 14., ...,  2., 22., 76.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  2., ...,  0.,  0.,  3.],\n",
       "        ...,\n",
       "        [ 4.,  1.,  7., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  1.,  2.,  5.],\n",
       "        [23.,  5.,  5., ...,  0.,  0.,  3.]], dtype=float32),\n",
       " array([[ 45.,  12.,   0., ...,   0.,   0.,  34.],\n",
       "        [ 97.,  52.,   1., ...,   0.,   1.,   3.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,  40.],\n",
       "        ...,\n",
       "        [  0.,   0.,   2., ...,   1.,   0.,  74.],\n",
       "        [100.,   3.,   0., ...,   3.,   2.,   4.],\n",
       "        [  5.,   3.,   0., ...,   0.,   0.,   0.]], dtype=float32),\n",
       " array([[ 31.,   4.,   3., ...,   7.,  21.,   2.],\n",
       "        [  0.,   0.,   3., ...,  51.,   3.,   1.],\n",
       "        [ 40.,   5.,   0., ...,   4.,   2.,  86.],\n",
       "        ...,\n",
       "        [ 29.,   1.,   0., ...,   1.,   0.,   0.],\n",
       "        [109.,   3.,   0., ...,   4.,   0.,   0.],\n",
       "        [134.,   2.,   0., ...,   0.,   3.,  17.]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptors_list = []\n",
    "\n",
    "for filename in image_filenames:\n",
    "    # Load the image\n",
    "    image = cv2.imread(filename)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect and compute SIFT features\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    # Store the descriptors in a list\n",
    "    descriptors_list.append(descriptors)\n",
    "\n",
    "# Perform further processing with the descriptors list\n",
    "# ... (e.g., clustering, classification, etc.)\n",
    "\n",
    "descriptors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1e518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05056f0a",
   "metadata": {},
   "source": [
    "### Feature Matching:\n",
    "You can match the keypoints and descriptors between different images to find correspondences. This can be useful for tasks like image stitching or object recognition. Here's an example of how you can match keypoints between two images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83fb5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two images\n",
    "image1 = cv2.imread(\"obama0.jpeg\")\n",
    "image2 = cv2.imread(\"obama5.jpeg\")\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect and compute SIFT features for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray_image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray_image2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match the descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw the matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None)\n",
    "\n",
    "# Display the matched image\n",
    "cv2.imshow('Matched Image', matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7652856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aef10cd",
   "metadata": {},
   "source": [
    "After performing feature matching using the SIFT algorithm, the next steps typically involve estimating the transformation between the matched features and applying the transformation to align the images. Here's an example of how you can proceed with image alignment using the matched features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e67ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "image1 = cv2.imread(\"obama0.jpeg\")\n",
    "image2 = cv2.imread(\"obama5.jpeg\")\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect and compute SIFT features for both images\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray_image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray_image2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match the descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_points = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_points = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate the transformation matrix using RANSAC\n",
    "ransac_thresh = 5.0\n",
    "M, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, ransac_thresh)\n",
    "\n",
    "# Apply the transformation to align the images\n",
    "aligned_image = cv2.warpPerspective(image2, M, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "# Display the aligned image\n",
    "cv2.imshow('Aligned Image', aligned_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3aa69",
   "metadata": {},
   "source": [
    "In this code, we first detect and compute the SIFT features for both images, just as before. Then, we perform feature matching using the BFMatcher and apply the ratio test to filter good matches.\n",
    "\n",
    "Next, we extract the matched keypoints from the good matches and reshape them into the required format for the findHomography function, which estimates the transformation matrix. RANSAC is used as a robust estimation method to handle outliers in the matches.\n",
    "\n",
    "With the estimated transformation matrix, we use the warpPerspective function to apply the transformation to the second image and align it with the first image. The resulting aligned image is displayed using imshow.\n",
    "\n",
    "It's important to note that the success of image alignment depends on the quality and number of good matches obtained. Additionally, you may need to handle cases where no or very few good matches are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fcc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bda020db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n",
      "Press any key to close the image window.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# List of image filenames\n",
    "image_filenames = [\"hellokitty.png\", \"hellokittyclothes.jpeg\", \"hellokittybag.jpeg\"]\n",
    "\n",
    "# Create a SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "for filename in image_filenames:\n",
    "    # Load the image\n",
    "    image = cv2.imread(filename)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect and compute SIFT features\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    # Draw the keypoints on the image\n",
    "    image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)\n",
    "\n",
    "    # Display the image with keypoints\n",
    "    cv2.imshow('Image with Keypoints', image_with_keypoints)\n",
    "    print(\"Press any key to close the image window.\")\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "# Close all image windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b84995ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two images\n",
    "image1 = cv2.imread(\"hellokittybag.jpeg\")\n",
    "image2 = cv2.imread(\"hellokittyclothes.jpeg\")\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect and compute SIFT features for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray_image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray_image2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match the descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw the matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None)\n",
    "\n",
    "# Display the matched image\n",
    "cv2.imshow('Matched Image', matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94eac459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "image1 = cv2.imread(\"hellokittybag.jpeg\")\n",
    "image2 = cv2.imread(\"hellokittyclothes.jpeg\")\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect and compute SIFT features for both images\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray_image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray_image2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match the descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_points = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_points = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate the transformation matrix using RANSAC\n",
    "ransac_thresh = 5.0\n",
    "M, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, ransac_thresh)\n",
    "\n",
    "# Apply the transformation to align the images\n",
    "aligned_image = cv2.warpPerspective(image2, M, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "# Display the aligned image\n",
    "cv2.imshow('Aligned Image', aligned_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093601fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
